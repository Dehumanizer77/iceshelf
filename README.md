# iceshelf

A simple tool to allow storage of private, incremental backups using Amazon's Glacier storage. It uses par2, tar, bzip2, gpg, json and nice stuff like that to accomplish the job.

# Features

- Encrypts all backups using GPG private/public key
- Signs all files it uploads (tamper detection)
- Can upload separate PAR2 file for parity correction
- Supports segmentation of upload (but not of files, yet)
- Primarily designed for AWS Glacier
- Tracks backups locally to help locate the file needed to restore
- Most features can be turned on/off and customized

Due to the need to work well with Glacier, any change to a file will cause it
to reupload the same file (with the new content). This backup solution is not
meant to be used on files which change often.

It's an archiving solution for long-term storage which is what Glacier excels
at. Also the reason it's called iceshelf. To quote from wikipedia:

> An ice shelf is a thick floating platform of ice that forms where a glacier or ice sheet flows down to a coastline and onto the ocean surface

*and yes, this would probably mean that time runs in reverse, but bear with
me, finding cool names (phun intended) for projects is not always easy*

# How does it all work?

1. Loads backup database if available
2. Empties prep directory of any files
3. Copies files to prep directory (recreating directory structure) until no more are found or limit is hit. If this wasn't the first run, only new or changed files are copied
4. A tar archive is created, possible compressed with bzip2 (depending on content and options)
5. The copies in the prep directory are deleted to save space
6. The archive is encrypted with a public key of your choice
7. The archive is signed with a public key of your choice (not necessarily the same as in #6)
8. A manifest of all files in the archive + checksums is stored as a JSON file
9. The manifest is signed (using ASCII instead of binary to keep it readable)
10. Parity file(s) are created to allow the archive to be restored should bitrot happen
11. All parity files are signed
12. Resulting files are uploaded to the cloud (may take a while with AWS Glacier)
13. Prep directory is emptied
14. New backup is added to local database
15. Local database is saved as JSON

A lot of things here can be customized, but in a nutshell, this is what the tool does.

All filenames generated by the tool are based on date and time (YYYYMMDD-HHMMSS-xxxxx, time is in UTC), which helps you figure out where data might hide if you need to find it and have lost the original local database. Also allows you to restore files in the *correct* order (since the tool may have more than one copy of the same file).

If you have the local database, you find that each file also points out which archive it belongs to. When a file is modified, it adds a new memberof entry. By sorting the backups field you can easily find the latest backup. Same applies to the an individual file, by sorting the memberof field you can find the latest version (or an old one).

## Requirements

In order to be able to run this, you need a few other parts installed.

- Python gnupg - Encryption & Signature
  Ubuntu comes with a version, but unfortunately it's too old. You should install this using the `pip` tool to make sure you get a current version.
- par2 - Parity tool
- glacier-cmd - The tool which interacts with Glacier
  This must be downloaded and installed manually, see

## Configuration file

Iceshelf requires a config file to work. You may name it whatever you want and it may exist wherever you want. The important part is that you point it out to the tool.

Here's what it all does...

### Section [sources]

Contains all the directories you wish to backup. Can also be individual files. You define each source by name=path/file, for example:

>my home movies=/mnt/storage/homemovies
>my little file=/mnt/documents/birthcertificate.pdf

*default is... no defined source*

### Section [paths]

Iceshelf needs some space for both temporary files and the local database.

#### prep dir

The folder to hold the temporary files, this needs to hold around twice the size of the maximum expected data to backed up at any time, so a ram-backed storage (such as tmpfs) is a **VERY BAD IDEA**. Especially since AWS Glacier uploads can take "a while".

But, you know, because I like breaking my own rules to prove them...

*default is "/tmp/"*

#### data dir

Where to store local data needed by iceshelf to function. Today that's a checksum database, tomorrow, who knows? Might be good to back up (yes, you can do that).

*default is "data/"*

### Section [options]

There are quite a options for you to play with. Unless otherwise specified, the options are toggled using "yes" or "no".

#### max size

Defines the maxium size of the *uncompressed* data. It will never go above this, but depending on various other options, the resulting backup files may exceed it.

This option is defined in bytes, but can also be suffixed with K, M, G or T to indicate the unit. We're using true powers of 2 here, so 1K = 1024.

A value of zero or simply blank (or left out) will make it unlimited (unless "add parity" is in-effect)

**If the backup didn't include all files due to exceeded max size, then iceshelf will exit with code 10. By rerunning iceshelf with the same parameters it will continue where it left of. If you do this until it exits with zero, you'll have a full backup.**

*default is blank, no limit*

#### change method

How to detect changes. You have two modes, either "meta" or "data". Meta uses the combination of modification time and file size. If either (or both) change, it's considered changed. This is very fast and effecient but could in theory miss changes. By using "data", iceshelf will use sha512 to generate a hash of the data which is then compared. This is more secure and will detect more or less all changes, *but*, it's also more computationally expensive and *will* make the process slower. It also adds a substancial amount of extra data due to larger checksums.

*default is "meta"*

#### delta manifest

Save a delta manifest with the archive as separate file. This is essentially a JSON file with the filenames and their checksums. Handy if you ever loose the entire local database since you can download all your manifests in order to locate the missing file.

Please keep in-mind that this is a *delta* manifest, it does not contain anything but the files in this backup, there are no references to any other files from previous backups.

*default is "yes"*

#### compress

Controlling compression, this option can be "yes", "no", "force". While "no" is obvious, "yes" is somewhat more clever. It will calculate how many of the files included in the backup are considered compressible (see "incompressible") and engage compression if 20% or more is considered compressible.

Now, "force" is probably more obvious, but we cover it anyway for completeness. It essentially overrides the logic of "yes" and compresses regardless.

*default is "yes"*

#### persuasive

While a fun name for an option, it essentially says that even if the next file won't fit within the max size limits, it should continue and see if any other file fits. This is to try and make sure that all archives are of equal size.

*default is "yes"*

#### ignore overlimit

If "yes", this will make iceshelf return a success code once all files are backed up, even if it has skipped files that are larger than the max size. So if you have 10 files and one is larger than max size, then 9 files will be backed up and it will still return OK (exit code 0), without this option, it would have failed and had a non-zero exit code.

*default is "no"*

#### incompressible

Using this option, you can add additional file extensions which will be considered incompressible by the built-in logic.

*default is blank, relying only on the built-in list*

### Section [exclude]

This is an optional section, by default iceshelf will backup every file it finds in the source. But sometimes that's not always appreciated. This section allows you to define some exclusion rules.

You define rules the same way you do sources, by name=rule, for example:

>no zip files=*.zip
>no cache=/home/user/cache
>...

In the simplest form, the rule is simply a definition of what the filename (including path) is starting with. If this matches, it's excluded. All rules are CaSe-InSeNsItIvE.

#### Prefixes

You can however make it more complex by using prefixes. By prefixing the rule with a star (*) the rule will match starting from the end. By prefixing with a questionmark (?) the rule will match any file containing the rule. Finally you can also use less-than or more-than (&lt; or &gt;) followed by a size to exclude by size only.

But wait, there's more. You can on top of these prefixes add an additional prefix (a pre-prefix) in the shape of an exclamationmark. This will *invert* the rule and make it inclusive instead.

Why would you want to do this?

Consider the following:
```
[exclude]
alldocs=!*.doc
no odd dirs=/some/odd/dir/
```

In a structure like this:

```
/some/
/some/data.txt
/some/todo.doc
/some/odd/dir/
/some/odd/dir/moredata.txt
/some/odd/dir/readme.doc
```

It will backup the following:

```
/some/data.txt
/some/todo.doc
/some/odd/dir/readme.doc
```

Notice how it snagged a file from inside an excluded folder? Pretty convenient. However, in order for this to work, you must consider the order of the rules. If you change the order to:

```
[exclude]
no odd dirs=/some/odd/dir/
alldocs=!*.doc
```

The "no odd dirs" would trigger first and the second rule would never get a chance to be evaluated. If you're having issues with the rules, consider running iceshelf with `--changes` and `--debug` to see what it's doing.

### Section [glacier]

This is, believe it or not, optional. Yes, you can run iceshelf locally and have it store the backup on whatever storage that the "done dir" option is pointing at. However, should you decide to use this for glacier, you'll first of all need to make sure that glacier-cmd is installed.

Once installed AND configured, you have two parameters that you need to set here to make it all work.

#### config

The configuration file for glacier-cmd, without this, it will not work. Even if you have defined the default config which glacier-cmd automatically uses, you still need to define it here or iceshelf will complain.

#### vault

The name of the vault. Iceshelf will automatically create the vault if it doesn't exist, it will also avoid doing so to minimize the operations towards the AWS to avoid extra fees. It does this by only creating/checking the existance of the vault when you run iceshelf the first time or when you change the vault name from its previous name.

### Section [security]

From here you can control everything which relates to security of the content and the parity controls. Make sure you have GPG installed or this will not function properly.

#### encrypt

Specifies the GPG key to use for encryption. Usually an email address. This option can be used independently from sign and can also use a different key.

Only the archive file is encrypted.

*default is blank*

#### encrypt phrase

If your encryption key needs a passphrase, this is the place you put it.

*default is blank*

#### sign

Specifies the GPG key to use for signing files. Usually an email address. This option can be used independently from encrypt and can also use a different key.

Using signature will sign *every* file associated with the archive, including the archive itself. It gives you the benefit of being able to validate the data as well as detecting if the archive has been damaged/tampered with.

See "add parity" for dealing with damaged archive files.

*default is blank*

#### sign phrase

If your signature key needs a passphrase, this is the place you put it.

*default is blank*

#### add parity

Adds a PAR2 parity file, allowing you to recover from errors in the archive, should that have happened. These files will never be encrypted, only signed if you've enabled signature. The value for this option is the percentage of errors in the archive that you wish to be able to deal with.

The value ranges from 0 (off) to 100 (the whole file).

Remember, if you ask for 50%, the resulting archive files *will* be roughly 50% larger.

For security people, this option is acting upon the already encrypted and signed version of the archive, so even at 100%, there won't be any data which can be used to get around the encryption.

There is unfortunately also a caveat with using parity. Due to a limitation of the PAR2 specification, "max size" will automatically be set to 32GB, regardless if you have set it to unlimited or >32GB.

*default is "0" due to the imposed limit*

## Commandline

You can also provide a few options via the commandline, these are not available in the configuration file.

`--changes` will show you what *would* be backed up, if you were to do it

`--logfile` redirects the log output to a separate file, otherwise warning and errors are shown on the console. Enabling file logging will also enable full debugging.

`--find <string>` will show any file and backup which contains the `<string>`

`--modified` shows files which have changed and the number of times

`--show <archive>` lists all files components which makes up a particular backup. This is refering to the archive file, manifest, etc. Not the contents of the actual backup. Helpful when you need to retreive a backup and you want to know all the files.

`--full` forces a complete backup, foregoing the incremential logic.

No matter what options you add, you *must* point out the configuration file, or you will not get any results.

# Current state

Right now, the tool will do everything except upload the files it generates.
Which is a bit of a letdown,. But I need to verify locally that the tool works
properly before I let it loose on AWS Glacier since it could get very costly.

# Thoughts

- Is par2 usage just paranoia?
- Better options than par2 which are open-source?
- JSON is probably not going to cut-it in the future for local metadata storage

# FAQ

## What about the local database?

Yes, it's vulnerable to tampering, bitrot and loss. But instead of constructing something to solve that locally, I would recommend you simply add an entry to the [sources] section of the config:

```
iceshelf-db=/where/i/store/the/checksum.json
```

And presto, each copy of the archive will have the previous database included. Which is fine because normally the "delta manifest" option is enabled which means that you got it all covered.

If this turns out to be a major concern/issue, I'll revisit this question.