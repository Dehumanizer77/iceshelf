# iceshelf

A simple tool to allow storage of private, incremental backups using Amazon's Glacier storage. It uses par2, tar, bzip2, gpg, json and nice stuff like that to accomplish the job.

# Features

- Encrypts all backups using GPG private/public key
- Signs all files it uploads (tamper detection)
- Can upload separate PAR2 file for parity correction
- Supports segmentation of upload (but not of files, yet)
- Primarily designed for AWS Glacier
- Tracks backups locally to help locate the file needed to restore
- Most features can be turned on/off and customized

Due to the need to work well with Glacier, any change to a file will cause it
to reupload the same file (with the new content). This backup solution is not
meant to be used on files which change often.

It's an archiving solution for long-term storage which is what Glacier excels
at. Also the reason it's called iceshelf. To quote from wikipedia:

> An ice shelf is a thick floating platform of ice that forms where a glacier or ice sheet flows down to a coastline and onto the ocean surface

*and yes, this would probably mean that time runs in reverse, but bear with
me, finding cool names (phun intended) for projects is not always easy*

# How does it all work?

1. Loads backup database if available
2. Empties prep directory of any files
3. Copies files to prep directory (recreating directory structure) until no more are found or limit is hit. If this wasn't the first run, only new or changed files are copied
4. A tar archive is created, possible compressed with bzip2 (depending on content and options)
5. The copies in the prep directory are deleted to save space
6. The archive is encrypted with a public key of your choice
7. The archive is signed with a public key of your choice (not necessarily the same as in #6)
8. A manifest of all files in the archive + checksums is stored as a JSON file
9. The manifest is signed (using ASCII instead of binary to keep it readable)
10. Parity file(s) are created to allow the archive to be restored should bitrot happen
11. All parity files are signed
12. Resulting files are uploaded to the cloud (may take a while with AWS Glacier)
13. Prep directory is emptied
14. New backup is added to local database
15. Local database is saved as JSON

A lot of things here can be customized, but in a nutshell, this is what the tool does.

All filenames generated by the tool are based on date and time (YYYYMMDD-HHMMSS-xxxxx, time is in UTC), which helps you figure out where data might hide if you need to find it and have lost the original local database. Also allows you to restore files in the *correct* order (since the tool may have more than one copy of the same file).

If you have the local database, you find that each file also points out which archive it belongs to. When a file is modified, it adds a new memberof entry. By sorting the backups field you can easily find the latest backup. Same applies to the an individual file, by sorting the memberof field you can find the latest version (or an old one).

## Configuration file

Iceshelf requires a config file to work. You may name it whatever you want and it may exist wherever you want. The important part is that you point it out to the tool.

Here's what it all does...

### Section [sources]

Contains all the directories you wish to backup. Can also be individual files. You define each source by name=path/file, for example:

my home movies=/mnt/storage/homemovies
my little file=/mnt/documents/birthcertificate.pdf

*default is... no defined source*

### Section [paths]

Iceshelf needs some space for both temporary files and the local database.

#### prep dir

The folder to hold the temporary files, this needs to hold around twice the size of the maximum expected data to backed up at any time, so a ram-backed storage (such as tmpfs) is a **VERY BAD IDEA**. Especially since AWS Glacier uploads can take "a while".

But, you know, because I like breaking my own rules to prove them...

*default is "/tmp/"*

#### data dir

Where to store local data needed by iceshelf to function. Today that's a checksum database, tomorrow, who knows? Might be good to back up (yes, you can do that).

*default is "data/"*

### Section [options]

There are quite a options for you to play with. Unless otherwise specified, the options are toggled using "yes" or "no".

#### max size

Defines the maxium size of the *uncompressed* data. It will never go above this, but depending on various other options, the resulting backup files may exceed it.

This option is defined in bytes, but can also be suffixed with K, M, G or T to indicate the unit. We're using true powers of 2 here, so 1K = 1024.

A value of zero or simply blank (or left out) will make it unlimited (unless "add parity" is in-effect)

*default is blank, no limit*

#### change method

How to detect changes. You have two modes, either "meta" or "data". Meta uses the combination of modification time and file size. If either (or both) change, it's considered changed. This is very fast and effecient but could in theory miss changes. By using "data", iceshelf will use sha512 to generate a hash of the data which is then compared. This is more secure and will detect more or less all changes, *but*, it's also more computationally expensive and *will* make the process slower. It also adds a substancial amount of extra data due to larger checksums.

*default is "meta"*

#### delta manifest

Save a delta manifest with the archive as separate file. This is essentially a JSON file with the filenames and their checksums. Handy if you ever loose the entire local database since you can download all your manifests in order to locate the missing file.

Please keep in-mind that this is a *delta* manifest, it does not contain anything but the files in this backup, there are no references to any other files from previous backups.

*default is "yes"*

#### compress

Controlling compression, this option can be "yes", "no", "force". While "no" is obvious, "yes" is somewhat more clever. It will calculate how many of the files included in the backup are considered compressible (see "incompressible") and engage compression if 20% or more is considered compressible.

Now, "force" is probably more obvious, but we cover it anyway for completeness. It essentially overrides the logic of "yes" and compresses regardless.

*default is "yes"*

#### persuasive

While a fun name for an option, it essentially says that even if the next file won't fit within the max size limits, it should continue and see if any other file fits. This is to try and make sure that all archives are of equal size.

*default is "yes"*

#### ignore overlimit

If "yes", this will make iceshelf return a success code once all files are backed up, even if it has skipped files that are larger than the max size. So if you have 10 files and one is larger than max size, then 9 files will be backed up and it will still return OK (exit code 0), without this option, it would have failed and had a non-zero exit code.

*default is "no"*

#### incompressible

Using this option, you can add additional file extensions which will be considered incompressible by the built-in logic.

*default is blank, relying only on the built-in list*

### Section [glacier]

TBD - May change name completely to better work with alternate backup solutions.

### Section [security]

From here you can control everything which relates to security of the content and the parity controls.

#### encrypt

Specifies the GPG key to use for encryption. Usually an email address. This option can be used independently from sign and can also use a different key.

Only the archive file is encrypted.

*default is blank*

#### encrypt phrase

If your encryption key needs a passphrase, this is the place you put it.

*default is blank*

#### sign

Specifies the GPG key to use for signing files. Usually an email address. This option can be used independently from encrypt and can also use a different key.

Using signature will sign *every* file associated with the archive, including the archive itself. It gives you the benefit of being able to validate the data as well as detecting if the archive has been damaged/tampered with.

See "add parity" for dealing with damaged archive files.

*default is blank*

#### sign phrase

If your signature key needs a passphrase, this is the place you put it.

*default is blank*

#### add parity

Adds a PAR2 parity file, allowing you to recover from errors in the archive, should that have happened. These files will never be encrypted, only signed if you've enabled signature. The value for this option is the percentage of errors in the archive that you wish to be able to deal with.

The value ranges from 0 (off) to 100 (the whole file).

Remember, if you ask for 50%, the resulting archive files *will* be roughly 50% larger.

For security people, this option is acting upon the already encrypted and signed version of the archive, so even at 100%, there won't be any data which can be used to get around the encryption.

There is unfortunately also a caveat with using parity. Due to a limitation of the PAR2 specification, "max size" will automatically be set to 32GB, regardless if you have set it to unlimited or >32GB.

*default is "0" due to the imposed limit*

## Commandline

You can also provide a few options via the commandline, these are not available in the configuration file.

`--changes` will show you what *would* be backed up, if you were to do it

`--logfile` redirects the log output to a separate file, otherwise warning and errors are shown on the console. Enabling file logging will also enable full debugging.

No matter what options you add, you *must* point out the configuration file, or you will not get any results.

# Current state

Right now, the tool will do everything except transfer the files it generates.
Which is a bit of a letdown. But I need to verify locally that the tool works
properly before I let it loose on AWS Glacier since it could get very costly.

# Roadmap

- Configurable loglevel
- Some good print-outs if running from commandline
- Quiet option to disable regular print-outs
- Add search functions for the local database
- Support alternative storage methods (such as cp, scp, rsync and similar)

# Thoughts

- Is par2 usage just paranoia?
- Better options than par2 which are open-source?

# FAQ

## What about the local database?

Yes, it's vulnerable to tampering, bitrot and loss. But instead of constructing something to solve that locally, I would recommend you simply add an entry to the [sources] section of the config:

> iceshelf-db=/where/i/store/the/checksum.json

And presto, each copy of the archive will have the previous database included. Which is fine because normally the "delta manifest" option is enabled which means that you got it all covered.

If this turns out to be a major concern/issue, I'll revisit this question.