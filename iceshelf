#!/usr/bin/env python
#
# Ice Shelf for Glacier is a incremental backup tool which will upload any
# changes to glacier. It can both encrypt and/or provide extra parity data
# to make sure that the data is secure and has some measure of protection
# against data corruption.
#
# Each backup can therefore be restored individually at the expense of
# extra storage in Glacier.
#
# Copyright (C) 2015 Henric Andersson (henric@sensenet.nu)
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#
#################################################################################

import logging
import argparse
import sys
import os.path
import hashlib
import json
from datetime import datetime
import time
import shutil
import gnupg
from subprocess import Popen, PIPE

import utils.config

oldFiles = {}
newFiles = {}
currentOp = {"filecount": 0, "filesize": 0, "compressable" : 0}

incompressable = [
  "jpg", "gif", "mkv", "avi", "mov", "mp4",
  "mp3", "flac", "zip", "bz2", "gz", "tgz",
  "7z", "aac", "rar", "vob", "m2ts", "ts",
  "jpeg", "psd", "png", "m4v", "m4a", "3gp",
  "tif", "tiff", "mts"
  ]

""" Parse command line """
parser = argparse.ArgumentParser(description="IceShelf - An Amazon Galcier Incremental backup tool", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('--logfile', metavar="FILE", help="Log to file instead of stdout")
parser.add_argument('--changes', action='store_true', default=False, help="Show changes to backup set but don't do anything")
parser.add_argument('config', metavar="CONFIG", help="Which config file to load")
cmdline = parser.parse_args()

""" Setup logging first """
logging.getLogger('').handlers = []
logging.basicConfig(filename=cmdline.logfile, level=logging.DEBUG, format='%(asctime)s - %(filename)s@%(lineno)d - %(levelname)s - %(message)s')
logging.getLogger("gnupg").setLevel(logging.WARNING)
logging.getLogger("shutil").setLevel(logging.WARNING)

#######################

def hashfile(file):
  print repr(hashlib.algorithms)
  sha = hashlib.new("sha512")
  with open(file, 'rb') as fp:
    for chunk in iter(lambda: fp.read(32768), b''):
      sha.update(chunk)
  return sha.hexdigest()

def shouldCompress():
  chance = int((currentOp["compressable"] * 100) / currentOp["filecount"])
  return chance > 90

def willCompress(filename):
  (ignore, ext) = os.path.splitext(filename)
  return ext[1:].lower() not in incompressable

def collectFile(filename):
  chksum = ""
  info = os.stat(filename)
  maxsize = config["maxsize"]

  if maxsize > 0 and info.st_size > maxsize:
    logging.warn("File \"%s\" is too big (%s) to ever fit inside defined max size of %s", filename, formatsize(info.st_size), formatsize(config["maxsize"]))
    return False

  if maxsize > 0 and (currentOp["filesize"] + info.st_size) > maxsize and not cmdline.changes:
    return False

  if config["use-sha"]:
    chksum = hashfile(filename)
  else:
    chksum = "%x-%x" % (info.st_size, info.st_mtime)

  if filename not in oldFiles or oldFiles[filename] != chksum:
    # When looking for changes, count files and size but don't process
    if cmdline.changes:
      currentOp["filecount"] += 1
      currentOp["filesize"] += info.st_size
      if filename not in oldFiles:
        logging.info("\"%s\" missing from backup", filename)
      else:
        logging.info("\"%s\" changed from last backup", filename)
      return False

    # Make sure path exists, then copy it...
    destfile = os.path.join(config["archivedir"], filename[1:])
    destdir = os.path.dirname(destfile)
    if not os.path.isdir(destdir):
      try:
        os.makedirs(destdir)
      except OSError as e:
        if e.errno is not 17:
          logging.exception("Error creating directory structure")
          raise

    # Copy the files into its new location
    shutil.copy2(filename, destfile)

    currentOp["filecount"] += 1
    currentOp["filesize"] += info.st_size
    if willCompress(filename):
      currentOp["compressable"] += 1
    newFiles[filename] = chksum

  return True

def collectSources(sources):
  # Time to start building a list of files
  result = True
  for name,path in sources.iteritems():
    logging.info("Processing \"%s\"", name)
    if os.path.isfile(path):
      if not collectFile(path):
        if not config["persuasive"] and not cmdline.changes:
          return False
        else:
          result = False
    else:
      for root, dirs, files in os.walk(path):
        for f in files:
          if not collectFile(os.path.join(root, f)):
            if not config["persuasive"] and not cmdline.changes:
              logging.debug("Not persuasive")
              return False
            else:
              result = False
  return result

def clearPrepDir():
  for root, dirs, files in os.walk(config["prepdir"], topdown=False):
    for name in files:
      os.remove(os.path.join(root, name))
    for name in dirs:
      os.rmdir(os.path.join(root, name))

def deleteArchiveSource():
  for root, dirs, files in os.walk(config["archivedir"], topdown=False):
    for name in files:
      os.remove(os.path.join(root, name))
    for name in dirs:
      os.rmdir(os.path.join(root, name))
  os.rmdir(config["archivedir"])

def generateParity(filename, level):
  if level is 0:
    return False
  cmd = ["par2", "create", "-r"+str(level), filename]
  p = Popen(cmd, stdout=PIPE, stderr=PIPE)
  out, err = p.communicate()
  if p.returncode != 0:
    print "Output: " + out
    print "Error : " + err
    print "Code  : " + str(p.returncode)
  return p.returncode == 0

def gatherData():
  mode = "tar"
  file_archive_plain = os.path.join(config["prepdir"], config["unique"])
  file_archive = file_archive_plain + ".tar"
  file_manifest = os.path.join(config["prepdir"], config["unique"]) + ".json"
  gpg = gnupg.GPG(options=['-z', '0']) # Do not use GPG compression since we use bzip2

  if config["compress"]:
    if config["compress-force"] or shouldCompress():
      mode = "bztar"
      file_archive += ".bz2"
    else:
      logging.info("Content is not likely to compress (%d%% chance), skipping compression.", shouldCompress())

  # Generate archive
  archive = shutil.make_archive(file_archive_plain, mode, config["archivedir"],logger=logging)
  if archive is None or archive == "":
    logging.error("Was unable to create archive")
    return None
  logging.info("Removing temporary copies of files")
  deleteArchiveSource()

  # Produce the manifest
  if config["manifest"]:
    with open(file_manifest, "w") as fp:
      json.dump(newFiles, fp)

  # Security for the archive
  if config["encrypt"]:
    logging.info("Encrypting archive")
    with open(file_archive, 'rb') as fp:
      gpg.encrypt_file(
        fp,
        config["encrypt"],
        passphrase=config["encrypt-pw"],
        armor=False,
        output=file_archive+".gpg"
      )
      # Remove old content
      os.remove(file_archive)
      file_archive += ".gpg"
  if config["sign"]:
    logging.info("Signing archive")
    with open(file_archive, 'rb') as fp:
      gpg.sign_file(
        fp,
        keyid=config["sign"],
        passphrase=config["sign-pw"],
        binary=True,
        output=file_archive+".sig"
      )
      # Remove old content
      os.remove(file_archive)
      file_archive += ".sig"

  # Add parity if requested
  if config["parity"] > 0 and not generateParity(file_archive, config["parity"]):
      logging.error("Unable to create PAR2 file for this archive")
      return None

  # Security for all companion files...
  if config["sign"]:
    if config["manifest"]:
      logging.info("Signing manifest")
      with open(file_manifest, 'rb') as fp:
        gpg.sign_file(
          fp,
          keyid=config["sign"],
          passphrase=config["sign-pw"],
          output=file_manifest+".asc"
        )
        # Remove old content
        os.remove(file_manifest)
        file_manifest += ".asc"
    if config["parity"] > 0:
      logging.info("Signing parity")
      for f in os.listdir(config["prepdir"]):
        if f.endswith('.par2'):
          f = os.path.join(config["prepdir"], f)
          with open(f, 'rb') as fp:
            data = gpg.sign_file(
              fp,
              keyid=config["sign"],
              passphrase=config["sign-pw"],
              binary=True,
              output=f+".sig"
            )
            # Remove old content
            os.remove(f)
  return os.listdir(config["prepdir"])

def sumsize(path, files):
  result = 0
  for f in files:
    result += os.path.getsize(os.path.join(path, f))
  return result

def formatsize(size):
  if size >= 1099511627776:
    return "%.1dT" % (size/1099511627776)
  if size >= 1073741824:
    return "%.1dG" % (size/1073741824)
  if size >= 1048576:
    return "%.1dM" % (size/1048576)
  if size >= 1024:
    return "%.1dK" % (size/1024)
  return str(size)

#####################

config = utils.config.parse(cmdline.config)
if config is None:
  logging.error("Configuration is broken, please check %s" % cmdline.config)
  sys.exit(1)

# Add more extensions (if provided)
if config["extra-ext"] is not None:
  incompressable += config["extra-ext"]

# Prep some needed config items which we generate
config["file-checksum"] = os.path.join(config["datadir"], "checksum.json")
tm = datetime.utcnow()
config["unique"] = "%d%02d%02d-%02d%02d%02d-%05x" % (tm.year, tm.month, tm.day, tm.hour, tm.minute, tm.second, tm.microsecond)
config["archivedir"] = os.path.join(config["prepdir"], config["unique"])

logging.info("Configuration is sane")

if os.path.exists(config["file-checksum"]):
  with open(config["file-checksum"], "r") as fp:
    oldSave = json.load(fp)
    oldFiles = oldSave["dataset"]
  logging.info(
    "Loaded previous checksums, last run was %s using version %s",
    datetime.fromtimestamp(oldSave["timestamp"]).strftime("%c"),
    oldSave["version"]
  )
else:
  logging.info("First run, no previous checksums")

logging.info("Clearing the prep directory")
clearPrepDir()

logging.info("Checking sources for changes")
gotall = collectSources(config['sources'])

# Don't continue, just give summary and a good exit-code
if cmdline.changes:
  if currentOp["filecount"] > 0:
    if len(oldFiles) == 0:
      msg = "%d files (%s) to be backed up"
    else:
      msg = "%d files (%s) has changed or been added since last backup"
    logging.info(msg, currentOp["filecount"], formatsize(currentOp["filesize"]))
    sys.exit(1)
  else:
    logging.info("No file(s) changed or added since last backup")
    sys.exit(0)

if len(newFiles) == 0 and not gotall:
  if config["ignore-overlimit"]:
    logging.info("Done. There were files which didn't fit the maxsize limit, but they were ignored")
    sys.exit(0)
  logging.error("Can't continue, you have files bigger than maxsize")
  sys.exit(255)

# Time to compress
files = gatherData()
if files is None:
  logging.error("Failed to gather all data and compress it.")
  sys.exit(2)

msg = "%d files (%s bytes) gathered" % (currentOp["filecount"], formatsize(currentOp["filesize"]))
if config["compress"] and (shouldCompress() or config["compress-force"]):
  msg += ", compressed"
if config["encrypt"]:
  msg += ", encrypted"
if config["sign"]:
  msg += ", signed"
msg += " and ready to upload as %d files, total %s bytes" % (len(files), formatsize(sumsize(config["prepdir"], files)))
logging.info(msg)

# merge
for k,v in newFiles.iteritems():
  oldFiles[k] = newFiles[k]

print "Saving the data..."
saveData = {
  "version" : "1.0.0",
  "timestamp" : time.time(),
  "dataset" : oldFiles
}
with open(config["file-checksum"], "w") as fp:
  json.dump(saveData, fp)

if not gotall:
  logging.warn("Reached size limit, recommend running again after this session")

print "Done!"
