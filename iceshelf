#!/usr/bin/env python
#
# Ice Shelf for Glacier is a incremental backup tool which will upload any
# changes to glacier. It can both encrypt and/or provide extra parity data
# to make sure that the data is secure and has some measure of protection
# against data corruption.
#
# Each backup can therefore be restored individually at the expense of
# extra storage in Glacier.
#
# Copyright (C) 2015 Henric Andersson (henric@sensenet.nu)
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#
#################################################################################

import logging
import argparse
import sys
import os.path
import hashlib
import json
from datetime import datetime
import time
import shutil
import gnupg
from subprocess import Popen, PIPE
import utils

oldFiles = {}
newFiles = {}
backupSets = {}
currentOp = {"filecount": 0, "filesize": 0, "compressable" : 0}
oldVault = None

incompressable = [
  "jpg", "gif", "mkv", "avi", "mov", "mp4",
  "mp3", "flac", "zip", "bz2", "gz", "tgz",
  "7z", "aac", "rar", "vob", "m2ts", "ts",
  "jpeg", "psd", "png", "m4v", "m4a", "3gp",
  "tif", "tiff", "mts"
  ]

""" Parse command line """
parser = argparse.ArgumentParser(description="IceShelf - An Amazon Galcier Incremental backup tool", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('--logfile', metavar="FILE", help="Log to file instead of stdout")
parser.add_argument('--debug', action='store_true', default=False, help='Adds more details to the log output')
parser.add_argument('--changes', action='store_true', default=False, help="Show changes to backup set but don't do anything")
parser.add_argument('--find', metavar='STRING', help='Searches the backup archive for files which contain string in name (case-insensitive)')
parser.add_argument('--show', metavar='ARCHIVE', help='Shows members of a certain archive')
parser.add_argument('config', metavar="CONFIG", help="Which config file to load")
cmdline = parser.parse_args()

""" Setup logging first """
logging.getLogger('').handlers = []
loglevel=logging.INFO
if cmdline.logfile:
  logformat='%(asctime)s - %(levelname)s - %(message)s'
else:
  logformat='%(message)s'
if cmdline.debug:
  logformat='%(asctime)s - %(filename)s@%(lineno)d - %(levelname)s - %(message)s'

logging.basicConfig(stream=sys.stdout, filename=cmdline.logfile, level=loglevel, format=logformat)
logging.getLogger("gnupg").setLevel(logging.WARNING)
logging.getLogger("shutil").setLevel(logging.WARNING)

#######################

def hashfile(file):
  print repr(hashlib.algorithms)
  sha = hashlib.new("sha512")
  with open(file, 'rb') as fp:
    for chunk in iter(lambda: fp.read(32768), b''):
      sha.update(chunk)
  return sha.hexdigest()

def shouldCompress():
  chance = int((currentOp["compressable"] * 100) / currentOp["filecount"])
  return chance >= 20

def willCompress(filename):
  (ignore, ext) = os.path.splitext(filename)
  return ext[1:].lower() not in incompressable

def collectFile(filename):
  chksum = ""
  info = os.stat(filename)
  maxsize = config["maxsize"]

  if maxsize > 0 and info.st_size > maxsize:
    logging.warn("File \"%s\" is too big (%s) to ever fit inside defined max size of %s", filename, formatsize(info.st_size), formatsize(config["maxsize"]))
    return False

  if maxsize > 0 and (currentOp["filesize"] + info.st_size) > maxsize and not cmdline.changes:
    return False

  if config["use-sha"]:
    chksum = hashfile(filename)
  else:
    chksum = "%x-%x" % (info.st_size, info.st_mtime)

  if filename not in oldFiles or oldFiles[filename]["checksum"] != chksum:
    # When looking for changes, count files and size but don't process
    if cmdline.changes:
      currentOp["filecount"] += 1
      currentOp["filesize"] += info.st_size
      if filename not in oldFiles:
        logging.info("\"%s\" missing from backup", filename)
      else:
        logging.info("\"%s\" changed from last backup", filename)
      return False

    # Make sure path exists, then copy it...
    destfile = os.path.join(config["archivedir"], filename[1:])
    destdir = os.path.dirname(destfile)
    if not os.path.isdir(destdir):
      try:
        os.makedirs(destdir)
      except OSError as e:
        if e.errno is not 17:
          logging.exception("Error creating directory structure")
          raise

    # Copy the files into its new location
    shutil.copy2(filename, destfile)

    currentOp["filecount"] += 1
    currentOp["filesize"] += info.st_size
    if willCompress(filename):
      currentOp["compressable"] += 1
    newFiles[filename] = {"checksum" : chksum, "memberof" : [config["unique"]]}

  return True

def collectSources(sources):
  # Time to start building a list of files
  result = True
  for name,path in sources.iteritems():
    logging.info("Processing \"%s\" (%s)", name, path)
    if os.path.isfile(path):
      if not collectFile(path):
        if not config["persuasive"] and not cmdline.changes:
          return False
        else:
          result = False
    else:
      for root, dirs, files in os.walk(path):
        for f in files:
          if not collectFile(os.path.join(root, f)):
            if not config["persuasive"] and not cmdline.changes:
              logging.debug("Not persuasive")
              return False
            else:
              result = False
  return result

def clearPrepDir():
  for root, dirs, files in os.walk(config["prepdir"], topdown=False):
    for name in files:
      os.remove(os.path.join(root, name))
    for name in dirs:
      os.rmdir(os.path.join(root, name))

def deleteArchiveSource():
  for root, dirs, files in os.walk(config["archivedir"], topdown=False):
    for name in files:
      os.remove(os.path.join(root, name))
    for name in dirs:
      os.rmdir(os.path.join(root, name))
  os.rmdir(config["archivedir"])

def generateParity(filename, level):
  if level is 0:
    return False
  cmd = ["par2", "create", "-r"+str(level), filename]
  p = Popen(cmd, stdout=PIPE, stderr=PIPE)
  out, err = p.communicate()
  if p.returncode != 0:
    print "Output: " + out
    print "Error : " + err
    print "Code  : " + str(p.returncode)
  return p.returncode == 0

def gatherData():
  mode = "tar"
  file_archive_plain = os.path.join(config["prepdir"], config["unique"])
  file_archive = file_archive_plain + ".tar"
  file_manifest = os.path.join(config["prepdir"], config["unique"]) + ".json"
  gpg = gnupg.GPG(options=['-z', '0']) # Do not use GPG compression since we use bzip2

  if config["compress"]:
    if config["compress-force"] or shouldCompress():
      mode = "bztar"
      file_archive += ".bz2"
    else:
      logging.info("Content is not likely to compress (%d%% chance), skipping compression.", shouldCompress())

  # Generate archive
  archive = shutil.make_archive(file_archive_plain, mode, config["archivedir"],logger=logging)
  if archive is None or archive == "":
    logging.error("Was unable to create archive")
    return None
  logging.info("Removing temporary copies of files")
  deleteArchiveSource()

  # Produce the manifest
  if config["manifest"]:
    with open(file_manifest, "w") as fp:
      json.dump(newFiles, fp)

  # Security for the archive
  if config["encrypt"]:
    logging.info("Encrypting archive")
    with open(file_archive, 'rb') as fp:
      gpg.encrypt_file(
        fp,
        config["encrypt"],
        passphrase=config["encrypt-pw"],
        armor=False,
        output=file_archive+".gpg"
      )
      # Remove old content
      os.remove(file_archive)
      file_archive += ".gpg"
  if config["sign"]:
    logging.info("Signing archive")
    with open(file_archive, 'rb') as fp:
      gpg.sign_file(
        fp,
        keyid=config["sign"],
        passphrase=config["sign-pw"],
        binary=True,
        output=file_archive+".sig"
      )
      # Remove old content
      os.remove(file_archive)
      file_archive += ".sig"

  # Add parity if requested
  if config["parity"] > 0 and not generateParity(file_archive, config["parity"]):
      logging.error("Unable to create PAR2 file for this archive")
      return None

  # Security for all companion files...
  if config["sign"]:
    if config["manifest"]:
      logging.info("Signing manifest")
      with open(file_manifest, 'rb') as fp:
        gpg.sign_file(
          fp,
          keyid=config["sign"],
          passphrase=config["sign-pw"],
          output=file_manifest+".asc"
        )
        # Remove old content
        os.remove(file_manifest)
        file_manifest += ".asc"
    if config["parity"] > 0:
      logging.info("Signing parity")
      for f in os.listdir(config["prepdir"]):
        if f.endswith('.par2'):
          f = os.path.join(config["prepdir"], f)
          with open(f, 'rb') as fp:
            data = gpg.sign_file(
              fp,
              keyid=config["sign"],
              passphrase=config["sign-pw"],
              binary=True,
              output=f+".sig"
            )
            # Remove old content
            os.remove(f)
  return os.listdir(config["prepdir"])

def sumsize(path, files):
  result = 0
  for f in files:
    result += os.path.getsize(os.path.join(path, f))
  return result

def formatsize(size):
  if size >= 1099511627776:
    return "%.1dT" % (size/1099511627776)
  if size >= 1073741824:
    return "%.1dG" % (size/1073741824)
  if size >= 1048576:
    return "%.1dM" % (size/1048576)
  if size >= 1024:
    return "%.1dK" % (size/1024)
  return str(size)

# TODO: This one should actually show output as it goes...
def glacierCommand(args):
  if config["glacier-config"] is None:
    logging.error("glacierCommand() called without proper settings")
    return None

  cmd = ["glacier-cmd", "-c", config["glacier-config"], "--output", "json"]
  cmd += args
  p = Popen(cmd, stdout=PIPE, stderr=PIPE)
  out, err = p.communicate()
  return {"code" : p.returncode, "output" : out, "error" : err }
"""
upload:
{'output': '{"Created archive with ID": "", "Archive SHA256 tree hash": "", "Uploaded file": ""}\n', 'code': 0, 'error': ''}

mkvault:
{"RequestId": "", "Location": "/5555555555/vaults/test"}

lsvault:
[{"SizeInBytes": 0, "LastInventoryDate": null, "VaultARN": "arn:aws:glacier:", "VaultName": "test", "NumberOfArchives": 0, "CreationDate": "2015-10-01T06:13:47.811Z"}]



"""

#####################

config = utils.parseConfig(cmdline.config)
if config is None:
  logging.error("Configuration is broken, please check %s" % cmdline.config)
  sys.exit(1)

# Add more extensions (if provided)
if config["extra-ext"] is not None:
  incompressable += config["extra-ext"]

# Prep some needed config items which we generate
config["file-checksum"] = os.path.join(config["datadir"], "checksum.json")
tm = datetime.utcnow()
config["unique"] = "%d%02d%02d-%02d%02d%02d-%05x" % (tm.year, tm.month, tm.day, tm.hour, tm.minute, tm.second, tm.microsecond)
config["archivedir"] = os.path.join(config["prepdir"], config["unique"])

"""
Load the old data, containing checksums and backup sets
"""
if os.path.exists(config["file-checksum"]):
  with open(config["file-checksum"], "r") as fp:
    oldSave = json.load(fp)
    oldFiles = oldSave["dataset"]
    backupSets = oldSave["backups"]
    oldVault = oldSave["vault"]
  logging.info(
    "State loaded, last run was %s using version %s",
    datetime.fromtimestamp(oldSave["timestamp"]).strftime("%c"),
    oldSave["version"]
  )
else:
  logging.info("First run, no previous checksums")

if cmdline.show:
  archive = cmdline.show.lower()
  if archive in backupSets:
    logging.info("Members of \"%s\":", archive)
    for f in backupSets[archive]:
      logging.info("  %s", f)
  else:
    logging.error("No such backup, \"%s\"", cmdline.show)
  sys.exit(0)

if cmdline.find:
  logging.info("Searching for \"%s\"", cmdline.find)
  found = 0
  for k,v in oldFiles.iteritems():
    if k.lower().find(cmdline.find.lower()) is not -1:
      logging.info("  \"%s\", exists in:", k)
      found += 1
      v["memberof"].sort()
      for x in v["memberof"]:
        logging.info("    %s", x)
  logging.info("Found %d instances", found)
  if found:
    sys.exit(0)
  else:
    sys.exit(1)


logging.info("Clearing the prep directory")
clearPrepDir()

logging.info("Checking sources for changes")
gotall = collectSources(config['sources'])

# Don't continue, just give summary and a good exit-code
if cmdline.changes:
  if currentOp["filecount"] > 0:
    if len(oldFiles) == 0:
      msg = "%d files (%s) to be backed up"
    else:
      msg = "%d files (%s) has changed or been added since last backup"
    logging.info(msg, currentOp["filecount"], formatsize(currentOp["filesize"]))
    sys.exit(1)
  else:
    logging.info("No file(s) changed or added since last backup")
    sys.exit(0)

if len(newFiles) == 0 and not gotall:
  if config["ignore-overlimit"]:
    logging.info("Done. There were files which didn't fit the maxsize limit, but they were ignored")
    sys.exit(0)
  logging.error("Can't continue, you have files bigger than maxsize")
  sys.exit(255)

# Time to compress
files = gatherData()
if files is None:
  logging.error("Failed to gather all data and compress it.")
  sys.exit(2)

msg = "%d files (%s bytes) gathered" % (currentOp["filecount"], formatsize(currentOp["filesize"]))
if config["compress"] and (shouldCompress() or config["compress-force"]):
  msg += ", compressed"
if config["encrypt"]:
  msg += ", encrypted"
if config["sign"]:
  msg += ", signed"
msg += " and ready to upload as %d files, total %s bytes" % (len(files), formatsize(sumsize(config["prepdir"], files)))
logging.info(msg)

##############################################################################
#

# We want to avoid wasting requests, so only try to
# create vaults if we need to.
if config["glacier-vault"] != oldVault:
  logging.info("Glacier vault most likely not in cloud, let's create it")

#
##############################################################################

# merge new files, checksums and memberships
for k,v in newFiles.iteritems():
  if k in oldFiles: # Don't forget any old memberships
    newFiles[k]["memberof"] += oldFiles[k]["memberof"]
  oldFiles[k] = newFiles[k]

# Add the backup to our sets...
backupSets[config["unique"]] = files

logging.info("Saving the new checksum")
saveData = {
  "version" : "1.0.0",
  "timestamp" : time.time(),
  "dataset" : oldFiles,
  "backups" : backupSets,
  "vault" : config["glacier-vault"]
}
with open(config["file-checksum"] + "_tmp", "w") as fp:
  json.dump(saveData, fp)

# Copy the new file into place and then delete the temp file
shutil.copy(config["file-checksum"] + "_tmp", config["file-checksum"])
os.remove(config["file-checksum"] + "_tmp")

if config["donedir"] is not None:
  logging.info("Moving backed up archive into done directory")
  dest = os.path.join(config["donedir"], config["unique"])
  os.mkdir(dest)
  for f in files:
    shutil.copy(
      os.path.join(config["prepdir"], f),
      os.path.join(dest, f)
    )
    os.remove(os.path.join(config["prepdir"], f))
  os.rmdir(config["prepdir"])

  # Finally, we count the number of stored archives and delete the
  # older ones exceeding the defined limit.
  if config["maxkeep"] > 0:
    archives = os.listdir(config["donedir"])
    archives.sort()
    logging.info("Told to keep %d archive(s), we have %d", config["maxkeep"], len(archives))
    while len(archives) > config["maxkeep"]:
      folder = archives.pop(0)
      logging.info("Deleting \"%s\"", folder)
      shutil.rmtree(os.path.join(config["donedir"], folder))

if not gotall:
  logging.warn("Reached size limit, recommend running again after this session")
  sys.exit(10)
sys.exit(0)