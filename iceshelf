#!/usr/bin/env python
#
# Ice Shelf for Glacier is a incremental backup tool which will upload any
# changes to glacier. It can both encrypt and/or provide extra parity data
# to make sure that the data is secure and has some measure of protection
# against data corruption.
#
# Each backup can therefore be restored individually at the expense of
# extra storage in Glacier.
#
# Copyright (C) 2015 Henric Andersson (henric@sensenet.nu)
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#
#################################################################################

import logging
import argparse
import sys
import os.path
import json
from datetime import datetime
import time
import shutil
import gnupg
from subprocess import Popen, PIPE

import configuration
import fileutils
import helper
import glacier

oldFiles = {}
newFiles = {}
allFiles = []
movedFiles = {}
deletedFiles = []
backupSets = {}
currentOp = {"filecount": 0, "filesize": 0, "compressable" : 0}
oldVault = None

incompressable = [
  "jpg", "gif", "mkv", "avi", "mov", "mp4",
  "mp3", "flac", "zip", "bz2", "gz", "tgz",
  "7z", "aac", "rar", "vob", "m2ts", "ts",
  "jpeg", "psd", "png", "m4v", "m4a", "3gp",
  "tif", "tiff", "mts"
  ]

""" Parse command line """
parser = argparse.ArgumentParser(description="IceShelf - An Amazon Galcier Incremental backup tool", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('--logfile', metavar="FILE", help="Log to file instead of stdout")
parser.add_argument('--debug', action='store_true', default=False, help='Adds more details to the log output')
parser.add_argument('--changes', action='store_true', default=False, help="Show changes to backup set but don't do anything")
parser.add_argument('--find', metavar='STRING', help='Searches the backup archive for files which contain string in name (case-insensitive)')
parser.add_argument('--show', metavar='ARCHIVE', help='Shows members of a certain archive')
parser.add_argument('--modified', action='store_true', default=False, help='Show all files which exists multiple times due to modifications')
parser.add_argument('--full', action='store_true', default=False, help='Full backup, regardless of changes to files')
parser.add_argument('config', metavar="CONFIG", help="Which config file to load")
cmdline = parser.parse_args()

""" Setup logging first """
logging.getLogger('').handlers = []
loglevel=logging.INFO
if cmdline.logfile:
  logformat='%(asctime)s - %(levelname)s - %(message)s'
else:
  logformat='%(message)s'
if cmdline.debug:
  loglevel=logging.DEBUG
  logformat='%(asctime)s - %(filename)s@%(lineno)d - %(levelname)s - %(message)s'

logging.basicConfig(stream=sys.stdout, filename=cmdline.logfile, level=loglevel, format=logformat)
logging.getLogger("gnupg").setLevel(logging.WARNING)
logging.getLogger("shutil").setLevel(logging.WARNING)

#######################

def shouldCompress():
  chance = int((currentOp["compressable"] * 100) / currentOp["filesize"])
  return chance >= 20

def willCompress(filename):
  (ignore, ext) = os.path.splitext(filename)
  return ext[1:].lower() not in incompressable

def collectFile(filename):
  chksum = ""
  info = os.stat(filename)
  maxsize = config["maxsize"]

  if maxsize > 0 and info.st_size > maxsize:
    logging.warn("File \"%s\" is too big (%s) to ever fit inside defined max size of %s", filename, helper.formatSize(info.st_size), helper.formatSize(config["maxsize"]))
    return False

  if maxsize > 0 and (currentOp["filesize"] + info.st_size) > maxsize and not cmdline.changes:
    return False

  if config["use-sha"]:
    chksum = fileutils.hashFile(filename, config["sha-type"])
  else:
    chksum = "%x-%x" % (info.st_size, info.st_mtime)

  # Always log all found files, so we can determine if any was deleted
  allFiles.append(filename)

  if filename not in oldFiles or oldFiles[filename]["checksum"] != chksum or cmdline.full:
    # When looking for changes, count files and size but don't process
    if cmdline.changes:
      currentOp["filecount"] += 1
      currentOp["filesize"] += info.st_size
      if filename not in oldFiles:
        logging.info("\"%s\" missing from backup", filename)
      else:
        logging.info("\"%s\" changed from last backup", filename)
      return False

    currentOp["filecount"] += 1
    currentOp["filesize"] += info.st_size
    if willCompress(filename):
      currentOp["compressable"] += info.st_size
    newFiles[filename] = {"checksum" : chksum, "memberof" : [config["unique"]], "deleted": []}

  return True

def collectSources(sources):
  # Time to start building a list of files
  result = True
  for name,path in sources.iteritems():
    logging.info("Processing \"%s\" (%s)", name, path)
    if os.path.isfile(path):
      if not configuration.isExcluded(path):
        if not collectFile(path):
          if not config["persuasive"] and not cmdline.changes:
            return False
          else:
            result = False
    else:
      for root, dirs, files in os.walk(path):
        for f in files:
          if not configuration.isExcluded(os.path.join(root, f)):
            if not collectFile(os.path.join(root, f)):
              if not config["persuasive"] and not cmdline.changes:
                logging.debug("Not persuasive")
                return False
              else:
                result = False
  return result


def gatherData():
  mode = "tar"
  file_archive_plain = os.path.join(config["prepdir"], config["prefix"] + config["unique"])
  file_archive = file_archive_plain + ".tar"
  file_manifest = os.path.join(config["prepdir"], config["prefix"] + config["unique"]) + ".json"
  gpg = gnupg.GPG(options=['-z', '0']) # Do not use GPG compression since we use bzip2

  if config["compress"]:
    if config["compress-force"] or shouldCompress():
      mode = "bztar"
      file_archive += ".bz2"
    else:
      logging.info("Content is not likely to compress (%d%% chance), skipping compression.", shouldCompress())

  # Copy the files...
  for k in newFiles:
    if k not in movedFiles:
      # Make sure path exists, then copy it...
      destfile = config["archivedir"] + "/" + k
      destdir = os.path.dirname(destfile)
      if not os.path.isdir(destdir):
        try:
          os.makedirs(destdir)
        except OSError as e:
          if e.errno is not 17:
            logging.exception("Error creating directory structure")
            raise

      # Copy the files into its new location
      shutil.copy2(k, destfile)

  # Generate archive if we have any files
  if (len(newFiles) - len(movedFiles)):
    archive = shutil.make_archive(file_archive_plain, mode, config["archivedir"],logger=logging)
    if archive is None or archive == "":
      logging.error("Was unable to create archive")
      return None
    logging.info("Removing temporary copies of files")
    fileutils.deleteTree(config["archivedir"], True)

  # Produce the manifest
  if config["manifest"]:
    tmp1 = {}
    tmp2 = []

    for k,v in newFiles.iteritems():
      if k not in movedFiles:
        tmp1[k] = v
    for k in deletedFiles:
      if k not in movedFiles.values():
        tmp2.append(k)

    manifest = {"modified" : tmp1, "deleted" : tmp2, "moved" : movedFiles}
    with open(file_manifest, "w") as fp:
      json.dump(manifest, fp)

  # Security for the archive
  if config["encrypt"]:
    logging.info("Encrypting archive")
    with open(file_archive, 'rb') as fp:
      gpg.encrypt_file(
        fp,
        config["encrypt"],
        passphrase=config["encrypt-pw"],
        armor=False,
        output=file_archive+".gpg"
      )
      # Remove old content
      os.remove(file_archive)
      file_archive += ".gpg"
  if config["sign"]:
    logging.info("Signing archive")
    with open(file_archive, 'rb') as fp:
      gpg.sign_file(
        fp,
        keyid=config["sign"],
        passphrase=config["sign-pw"],
        binary=True,
        output=file_archive+".sig"
      )
      # Remove old content
      os.remove(file_archive)
      file_archive += ".sig"

  # Add parity if requested
  if config["parity"] > 0 and not fileutils.generateParity(file_archive, config["parity"]):
      logging.error("Unable to create PAR2 file for this archive")
      return None

  # Security for all companion files...
  if config["sign"]:
    if config["manifest"]:
      logging.info("Signing manifest")
      with open(file_manifest, 'rb') as fp:
        gpg.sign_file(
          fp,
          keyid=config["sign"],
          passphrase=config["sign-pw"],
          output=file_manifest+".asc"
        )
        # Remove old content
        os.remove(file_manifest)
        file_manifest += ".asc"
    if config["parity"] > 0:
      logging.info("Signing parity")
      for f in os.listdir(config["prepdir"]):
        if f.endswith('.par2'):
          f = os.path.join(config["prepdir"], f)
          with open(f, 'rb') as fp:
            data = gpg.sign_file(
              fp,
              keyid=config["sign"],
              passphrase=config["sign-pw"],
              binary=True,
              output=f+".sig"
            )
            # Remove old content
            os.remove(f)
  return os.listdir(config["prepdir"])


#####################

config = configuration.parse(cmdline.config)
if config is None:
  logging.error("Configuration is broken, please check %s" % cmdline.config)
  sys.exit(1)

# Add more extensions (if provided)
if config["extra-ext"] is not None:
  incompressable += config["extra-ext"]

# Prep some needed config items which we generate
config["file-checksum"] = os.path.join(config["datadir"], "checksum.json")
tm = datetime.utcnow()
config["unique"] = "%d%02d%02d-%02d%02d%02d-%05x" % (tm.year, tm.month, tm.day, tm.hour, tm.minute, tm.second, tm.microsecond)
config["archivedir"] = os.path.join(config["prepdir"], config["unique"])

"""
Load the old data, containing checksums and backup sets
"""
if os.path.exists(config["file-checksum"]):
  with open(config["file-checksum"], "r") as fp:
    oldSave = json.load(fp)
    oldFiles = oldSave["dataset"]
    backupSets = oldSave["backups"]
    oldVault = oldSave["vault"]
  logging.info(
    "State loaded, last run was %s using version %s",
    datetime.fromtimestamp(oldSave["timestamp"]).strftime("%c"),
    oldSave["version"]
  )
else:
  logging.info("First run, no previous checksums")

if cmdline.modified:
  found = 0
  logging.info("Searching for modified files:")
  for k,v in oldFiles.iteritems():
    if len(v["memberof"]) > 1:
      found += 1
      logging.info("\"%s\" modified %d times", k, len(v["memberof"]))
  logging.info("Found %d files (of %d) which have been modified", found, len(oldFiles))
  if found:
    sys.exit(0)
  else:
    sys.exit(1)

if cmdline.show:
  archive = cmdline.show.lower()
  if archive in backupSets:
    logging.info("Members of \"%s\":", archive)
    for f in backupSets[archive]:
      logging.info("  %s", f)
  else:
    logging.error("No such backup, \"%s\"", cmdline.show)
  sys.exit(0)

if cmdline.find:
  logging.info("Searching for \"%s\"", cmdline.find)
  found = 0
  for k,v in oldFiles.iteritems():
    if k.lower().find(cmdline.find.lower()) is not -1:
      logging.info("  \"%s\", exists in:", k)
      found += 1
      v["memberof"].sort()
      for x in v["memberof"]:
        logging.info("    %s", x)
  logging.info("Found %d instances", found)
  if found:
    sys.exit(0)
  else:
    sys.exit(1)


logging.info("Setting up the prep directory")
try:
  os.makedirs(config["prepdir"])
except OSError as e:
  if e.errno is not 17:
    logging.exception("Error creating prep directory")
    raise

fileutils.deleteTree(config["prepdir"])

logging.info("Checking sources for changes")
gotall = collectSources(config['sources'])

# Figure out which files that "got away" (ie, deleted)
for k,v in oldFiles.iteritems():
  if v["checksum"] != "" and k not in allFiles:
    deletedFiles.append(k)
allFiles = None

# Don't continue, just give summary and a good exit-code
if cmdline.changes:
  if currentOp["filecount"] > 0 or len(deletedFiles):
    if len(oldFiles) == 0:
      logging.info("%d files (%s) to be backed up", currentOp["filecount"], helper.formatSize(currentOp["filesize"]))
    else:
      logging.info("%d files (%s) has changed or been added since last backup, %d has been deleted", currentOp["filecount"], helper.formatSize(currentOp["filesize"]), len(deletedFiles))
    sys.exit(1)
  else:
    logging.info("No file(s) changed or added since last backup")
    sys.exit(0)

if len(newFiles) == 0 and not gotall:
  if config["ignore-overlimit"]:
    logging.info("Done. There were files which didn't fit the maxsize limit, but they were ignored")
    sys.exit(0)
  logging.error("Can't continue, you have files bigger than maxsize")
  sys.exit(3)

"""
Figure out if any file was renamed, this is easily detected since a deleted file
will have a new file with the same checksum. This does not work with meta data
"""
if config["use-sha"] and config["detect-move"]:
  for f in deletedFiles:
    chksum = oldFiles[f]["checksum"]
    for k,v in newFiles.iteritems():
      if v["checksum"] == chksum:
        movedFiles[k] = f # new place = old place
        #logging.info("File \"%s\" has been moved to \"%s\"", f, k)
        break

# Time to compress
files = gatherData()
if files is None:
  logging.error("Failed to gather all data and compress it.")
  sys.exit(2)

msg = "%d files (%s bytes) gathered" % (currentOp["filecount"], helper.formatSize(currentOp["filesize"]))
if config["compress"] and (shouldCompress() or config["compress-force"]):
  msg += ", compressed"
if config["encrypt"]:
  msg += ", encrypted"
if config["sign"]:
  msg += ", signed"
totalbytes = fileutils.sumSize(config["prepdir"], files)
msg += " and ready to upload as %d files, total %s" % (len(files), helper.formatSize(totalbytes))
logging.info(msg)

##############################################################################
#

# We want to avoid wasting requests, so only try to
# create vaults if we need to.
backup = False
if config["glacier-config"] is not None:
  if config["glacier-vault"] != oldVault or True:
    logging.info("Glacier vault most likely not in cloud, let's create it")
    if glacier.createVault(config):
      backup = glacier.uploadFiles(config, files, totalbytes)
  else:
    backup = glacier.uploadFiles(config, files, totalbytes)
else:
  logging.info("Not uploading to Glacier since config is missing")
  backup = True

if not backup:
  logging.error("Amazon Glacier upload failed, aborting")
  sys.exit(1)

#
##############################################################################

# merge new files, checksums and memberships
for k,v in newFiles.iteritems():
  if k in oldFiles: # Don't forget any old memberships
    newFiles[k]["memberof"] += oldFiles[k]["memberof"]
    if "deleted" in oldFiles[k]:
      newFiles[k]["deleted"] += oldFiles[k]["deleted"]
  oldFiles[k] = newFiles[k]

"""
Deal with deleted files. We must store all deletes as an array since user can
restore the file. We also must wipe the checksum so a restored file gets backed
up again.
"""
for f in deletedFiles:
  if "deleted" in oldFiles[f]:
    oldFiles[f]["deleted"].append(config["unique"])
  else:
    oldFiles[f]["deleted"] = [config["unique"]]
  oldFiles[f]["checksum"] = "" # Wipe checksum to make sure new copy is backed up

# Add the backup to our sets...
backupSets[config["unique"]] = files

logging.info("Saving the new checksum")
saveData = {
  "version" : "1.0.1",
  "timestamp" : time.time(),
  "dataset" : oldFiles,
  "backups" : backupSets,
  "vault" : config["glacier-vault"]
}
with open(config["file-checksum"] + "_tmp", "w") as fp:
  json.dump(saveData, fp)

# Copy the new file into place and then delete the temp file
shutil.copy(config["file-checksum"] + "_tmp", config["file-checksum"])
os.remove(config["file-checksum"] + "_tmp")

if config["donedir"] is not None:
  logging.info("Moving backed up archive into done directory")
  dest = os.path.join(config["donedir"], config["unique"])
  os.mkdir(dest)
  for f in files:
    shutil.copy(
      os.path.join(config["prepdir"], f),
      os.path.join(dest, f)
    )
    os.remove(os.path.join(config["prepdir"], f))
  os.rmdir(config["prepdir"])

  # Finally, we count the number of stored archives and delete the
  # older ones exceeding the defined limit.
  if config["maxkeep"] > 0:
    archives = os.listdir(config["donedir"])
    archives.sort()
    logging.info("Told to keep %d archive(s), we have %d", config["maxkeep"], len(archives))
    while len(archives) > config["maxkeep"]:
      folder = archives.pop(0)
      logging.info("Deleting \"%s\"", folder)
      shutil.rmtree(os.path.join(config["donedir"], folder))

if not gotall:
  logging.warn("Reached size limit, recommend running again after this session")
  sys.exit(10)
sys.exit(0)
